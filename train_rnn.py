# train_rnn.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import pickle
import time
import json
import os
from data_loader import NMTDataset
from models import build_model

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ----------------------------
# 1. åŠ è½½è¯æ±‡è¡¨
# ----------------------------
with open('data/prepared_vocab_zh.pkl', 'rb') as f:
    vocab_zh = pickle.load(f)  # word -> index

with open('data/prepared_vocab_en.pkl', 'rb') as f:
    vocab_en = pickle.load(f)

src_vocab_size = len(vocab_zh)
tgt_vocab_size = len(vocab_en)
src_pad_idx = vocab_zh['<pad>']  # åº”è¯¥æ˜¯ 0
tgt_pad_idx = vocab_en['<pad>']  # åº”è¯¥æ˜¯ 0

print(f"âœ… Chinese vocab size: {src_vocab_size}, pad_idx: {src_pad_idx}")
print(f"âœ… English vocab size: {tgt_vocab_size}, pad_idx: {tgt_pad_idx}")

# ----------------------------
# 2. é…ç½®
# ----------------------------
config = {
    'model_type': 'rnn',
    'src_vocab_size': src_vocab_size,
    'tgt_vocab_size': tgt_vocab_size,
    'embed_dim': 256,
    'hidden_dim': 512,
    'num_layers': 2,
    'attn_type': 'additive',
    'dropout': 0.3,
    'batch_size': 32,
    'epochs': 10,
    'lr': 0.001,
    'teacher_forcing_ratio': 0.5,
    'src_pad_idx': src_pad_idx,
    'tgt_pad_idx': tgt_pad_idx,
}

# ----------------------------
# 3. æ¨¡å‹ & ä¼˜åŒ–å™¨
# ----------------------------
model = build_model(config, device).to(device)
optimizer = optim.Adam(model.parameters(), lr=config['lr'])
criterion = nn.CrossEntropyLoss(ignore_index=tgt_pad_idx)  # å¿½ç•¥ <pad> (index=0)

# ----------------------------
# 4. æ•°æ®åŠ è½½
# ----------------------------
train_dataset = NMTDataset('data/prepared_train.pt', src_pad_idx, tgt_pad_idx)
train_loader = DataLoader(
    train_dataset,
    batch_size=config['batch_size'],
    shuffle=True,
    collate_fn=lambda b: collate_fn(b, src_pad_idx, tgt_pad_idx)
)

# ä¿®æ”¹ collate_fn æ”¯æŒè‡ªå®šä¹‰ pad_idx
def collate_fn(batch, src_pad_idx=0, tgt_pad_idx=0):
    src_seqs, tgt_seqs = zip(*batch)
    src_padded = torch.nn.utils.rnn.pad_sequence(src_seqs, batch_first=True, padding_value=src_pad_idx)
    tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_seqs, batch_first=True, padding_value=tgt_pad_idx)
    return src_padded, tgt_padded
# ----------------------------
# 5. è®­ç»ƒå‡½æ•°
# ----------------------------
def train_epoch(model, dataloader, optimizer, criterion, tf_ratio, device):
    model.train()
    total_loss = 0
    for src, tgt in dataloader:
        src = src.to(device)
        tgt = tgt.to(device)

        optimizer.zero_grad()
        output = model(src, tgt, teacher_forcing_ratio=tf_ratio)  # [B, L, vocab]

        # é¢„æµ‹ tgt[1:]ï¼Œå³è·³è¿‡ <sos>
        output = output[:, 1:].reshape(-1, output.shape[-1])
        tgt = tgt[:, 1:].reshape(-1)

        loss = criterion(output, tgt)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        total_loss += loss.item()
    return total_loss / len(dataloader)

# 6. ä¸»å¾ªç¯
# ----------------------------
if __name__ == "__main__":
    print("ğŸš€ Start training RNN-based Zhâ†’En NMT...")
    
    # â• æ–°å¢ï¼šå®šä¹‰è¾“å‡ºç›®å½•
    output_dir = "outputs/rnn"
    os.makedirs(output_dir, exist_ok=True)  # è‡ªåŠ¨åˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    
    train_losses = []

    for epoch in range(config['epochs']):
        start = time.time()
        loss = train_epoch(model, train_loader, optimizer, criterion, config['teacher_forcing_ratio'], device)
        print(f"Epoch {epoch+1}/{config['epochs']} | Loss: {loss:.4f} | Time: {time.time()-start:.2f}s")
        train_losses.append(loss)

    # â• ä¿å­˜æ¨¡å‹åˆ° outputs/rnn/
    model_path = os.path.join(output_dir, "model.pth")
    torch.save(model.state_dict(), model_path)
    print(f"âœ… Model saved to {model_path}")

    # â• ä¿å­˜è®­ç»ƒæ—¥å¿—åˆ° outputs/rnn/
    log_path = os.path.join(output_dir, "train_log.json")
    log_data = {
        'model_type': 'rnn',
        'epochs': config['epochs'],
        'train_losses': train_losses,
        'final_loss': train_losses[-1],
        'config': config  # å¯é€‰ï¼šä¿å­˜é…ç½®ä»¥ä¾¿å¤ç°å®éªŒ
    }
    with open(log_path, 'w') as f:
        json.dump(log_data, f, indent=4)
    print(f"âœ… Training log saved to {log_path}")